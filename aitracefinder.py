# -*- coding: utf-8 -*-
"""AITraceFinder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PLD90-Y2e4yQUeJ7EVYSv2mUhjlb2QPX

# **Mount The Drive**
"""

import os

# Base path
base_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"

# List everything inside the dataset folder
print("üìÇ Contents of AI_Trace_Finder_Datasets:")
print(os.listdir(base_path))

"""# **File Availability Check**"""

from google.colab import drive
import os

# Mount Drive (skip if already mounted)
drive.mount('/content/drive', force_remount=True)

# Base dataset path
base_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"

# Objective 1: Scanner Identification
flatfield_path = os.path.join(base_path, "Flatfield")
official_docs_path = os.path.join(base_path, "Official")
wiki_docs_path = os.path.join(base_path, "Wikipedia")

# Objective 2: Forgery Detection
tampered_path = os.path.join(base_path, "Tampered images")

# Verify folder contents
print("üìÇ Objective 1 Folders:")
print("Flatfield:", os.listdir(flatfield_path)[:5])
print("Official:", os.listdir(official_docs_path)[:5])
print("Wikipedia:", os.listdir(wiki_docs_path)[:5])

print("\nüìÇ Objective 2 Folders:")
print("Tampered:", os.listdir(tampered_path)[:5])

"""# **Checking Content inside files**"""

import os
from google.colab import drive

# Mount Drive (skip if already mounted)
drive.mount('/content/drive', force_remount=True)

# Base dataset path
base_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"

# Objective 1: Scanner Identification
flatfield_path = os.path.join(base_path, "Flatfield")
official_docs_path = os.path.join(base_path, "Official")
wiki_docs_path = os.path.join(base_path, "Wikipedia")

# Objective 2: Forgery Detection (only Description + Tampered)
tampered_desc_path = os.path.join(base_path, "Tampered images", "Description")
tampered_imgs_path = os.path.join(base_path, "Tampered images", "Tampered")

# ==============================
# Verify paths
# ==============================

print("Flatfield:", os.listdir(flatfield_path)[:5])
print("Official:", os.listdir(official_docs_path)[:5])
print("Wikipedia:", os.listdir(wiki_docs_path)[:5])


print("Description:", os.listdir(tampered_desc_path)[:5])
print("Tampered:", os.listdir(tampered_imgs_path)[:5])

"""# **PRNU Extraction Pipeline Code**"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import skew, kurtosis, entropy
from google.colab import drive

# ==========================
# 1. Mount Google Drive
# ==========================
drive.mount('/content/drive')

# Base dataset path
base_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"

# Objective 1 folders
flatfield_path = os.path.join(base_path, "Flatfield")
official_docs_path = os.path.join(base_path, "Official")
wiki_docs_path = os.path.join(base_path, "Wikipedia")

# Output path
output_path = os.path.join(base_path, "Objective1_PRNU_Output")
os.makedirs(output_path, exist_ok=True)

print("üìÇ Mounted dataset structure:")
print("Flatfield ‚Üí", os.listdir(flatfield_path)[:5])
print("Official ‚Üí", os.listdir(official_docs_path)[:5])
print("Wikipedia ‚Üí", os.listdir(wiki_docs_path)[:5])

# ==========================
# 2. Helper: PRNU Extraction
# ==========================
def extract_prnu(image_path):
    """Extracts PRNU fingerprint from a grayscale image."""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return None
    img = cv2.resize(img, (512, 512))  # normalize size

    # Wavelet denoising to get noise residual
    denoised = cv2.fastNlMeansDenoising(img, None, 10, 7, 21)
    residual = img.astype(np.float32) - denoised.astype(np.float32)

    # Normalize PRNU
    residual -= np.mean(residual)
    residual /= (np.std(residual) + 1e-6)

    return residual

# ==========================
# 3. Compute Fingerprints
# ==========================
scanner_fingerprints = {}

for scanner in os.listdir(flatfield_path):
    scanner_folder = os.path.join(flatfield_path, scanner)
    if not os.path.isdir(scanner_folder):
        continue

    residuals = []
    for fname in os.listdir(scanner_folder):
        img_path = os.path.join(scanner_folder, fname)
        prnu = extract_prnu(img_path)
        if prnu is not None:
            residuals.append(prnu)

    if residuals:
        fingerprint = np.mean(residuals, axis=0)
        scanner_fingerprints[scanner] = fingerprint

        # Save per-scanner fingerprint
        np.save(os.path.join(output_path, f"{scanner}_fingerprint.npy"), fingerprint)

        plt.imshow(fingerprint, cmap='gray')
        plt.title(f"{scanner} PRNU Fingerprint")
        plt.axis("off")
        plt.savefig(os.path.join(output_path, f"{scanner}_fingerprint.png"))
        plt.close()

print("‚úÖ Extracted PRNU fingerprints for", len(scanner_fingerprints), "scanners")

# ==========================
# 4. Save Outputs
# ==========================

# (A) Save flattened PRNUs to CSV for ML
df_full = pd.DataFrame(
    {scanner: fp.flatten() for scanner, fp in scanner_fingerprints.items()}
)
df_full.to_csv(os.path.join(output_path, "Scanner_Fingerprints.csv"), index=False)
print("üìÑ Saved full fingerprints (CSV, ML-ready)")

# (B) Save compact features to Excel (Excel-safe)
feature_records = []
for scanner, fingerprint in scanner_fingerprints.items():
    flat = fingerprint.flatten()

    mean_val = np.mean(flat)
    std_val = np.std(flat)
    energy_val = np.sum(flat**2)
    skew_val = skew(flat)
    kurt_val = kurtosis(flat)
    entropy_val = entropy(np.histogram(flat, bins=256)[0] + 1e-6)

    feature_records.append([scanner, mean_val, std_val, energy_val, skew_val, kurt_val, entropy_val])

df_features = pd.DataFrame(
    feature_records,
    columns=["Scanner", "Mean", "StdDev", "Energy", "Skewness", "Kurtosis", "Entropy"]
)
df_features.to_excel(os.path.join(output_path, "Scanner_Fingerprint_Features.xlsx"), index=False)
print("üìä Saved compact features (Excel-friendly)")

# (C) Combined visualization of all scanners
cols = 3
rows = int(np.ceil(len(scanner_fingerprints) / cols))
plt.figure(figsize=(15, 5 * rows))

for i, (scanner, fp) in enumerate(scanner_fingerprints.items(), 1):
    plt.subplot(rows, cols, i)
    plt.imshow(fp, cmap='gray')
    plt.title(scanner)
    plt.axis("off")

plt.tight_layout()
plt.savefig(os.path.join(output_path, "All_Scanners_Fingerprints.png"))
plt.close()

print("‚úÖ All outputs generated in:", output_path)

import os

out_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Objective1_PRNU_Output"
print("Files in output folder:", os.listdir(out_path))

import os
import shutil

out_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Objective1_PRNU_Output"

old_file = os.path.join(out_path, "Scanner_Fingerprint_Features.xlsx")
new_file = os.path.join(out_path, "Scanner_Fingerprints_Compact.xlsx")

# Rename (copy with new name)
if os.path.exists(old_file):
    shutil.copy(old_file, new_file)
    print(f"‚úÖ File renamed: {new_file}")
else:
    print("‚ùå Source file not found:", old_file)

import pandas as pd
import os

# Path to output folder
out_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Objective1_PRNU_Output"

# Load the compact Excel file
excel_file = os.path.join(out_path, "Scanner_Fingerprints_Compact.xlsx")

if os.path.exists(excel_file):
    df_preview = pd.read_excel(excel_file)
    print("‚úÖ Scanner Fingerprints Compact Preview:")
    display(df_preview.head())   # show first 5 rows nicely
else:
    print("‚ö†Ô∏è Compact Excel file not found:", excel_file)

"""# **Display all fingerprint images**"""

import os
import glob
import matplotlib.pyplot as plt
import pandas as pd
from PIL import Image

# Paths
out_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Objective1_PRNU_Output"
excel_file = os.path.join(out_path, "Scanner_Fingerprints_Compact.xlsx")

# --- 1. Load and display all fingerprint images ---
fingerprint_files = sorted(glob.glob(os.path.join(out_path, "*_fingerprint.png")))

num_images = len(fingerprint_files)
cols = 3
rows = (num_images + cols - 1) // cols  # calculate rows for grid

plt.figure(figsize=(15, rows * 4))
for i, file in enumerate(fingerprint_files):
    img = Image.open(file)
    plt.subplot(rows, cols, i + 1)
    plt.imshow(img, cmap="gray")
    plt.title(os.path.basename(file).replace("_fingerprint.png", ""))
    plt.axis("off")
plt.tight_layout()
plt.show()

print(f"‚úÖ Displayed {num_images} scanner fingerprints")

# --- 2. Load and preview Excel file ---
if os.path.exists(excel_file):
    df = pd.read_excel(excel_file)
    print("\n‚úÖ Scanner Fingerprints Features Table:")
    display(df)  # full table in Colab
else:
    print("‚ö†Ô∏è Excel file not found:", excel_file)

"""# **Preprocessing PRNU Dataset.**"""

import os
import numpy as np
import cv2
import pandas as pd
import matplotlib.pyplot as plt

# ==============================
# Step 2: Preprocessing PRNU Dataset
# ==============================

# Input & Output paths
input_folder = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Objective1_PRNU_Output"
output_folder = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Objective2_PRNU_Preprocessed"
os.makedirs(output_folder, exist_ok=True)

# Target resize shape
TARGET_SIZE = (256, 256)

# Dictionary to hold scanner-wise fingerprints
scanner_fingerprints = {}

# Step 1: Load all .npy fingerprint files
for file in os.listdir(input_folder):
    if file.endswith(".npy") and "fingerprint" in file:
        scanner_name = file.split("_fingerprint")[0]  # e.g., Canon120-1 ‚Üí Canon120
        scanner_name = scanner_name.split("-")[0]    # handle multiple samples per scanner

        file_path = os.path.join(input_folder, file)
        fingerprint = np.load(file_path)

        # Step 2: Resize
        resized = cv2.resize(fingerprint, TARGET_SIZE)

        # Step 3: Normalize (0‚Äì1)
        normalized = (resized - resized.min()) / (resized.max() - resized.min() + 1e-8)

        # Store in dict
        if scanner_name not in scanner_fingerprints:
            scanner_fingerprints[scanner_name] = []
        scanner_fingerprints[scanner_name].append(normalized)

# Step 4: Aggregate per scanner (average)
aggregated_results = {}
for scanner, fp_list in scanner_fingerprints.items():
    aggregated_fp = np.mean(fp_list, axis=0)  # average fingerprints
    aggregated_results[scanner] = aggregated_fp

    # Save aggregated .npy
    np.save(os.path.join(output_folder, f"{scanner}_aggregated.npy"), aggregated_fp)

    # Save visualization
    plt.imshow(aggregated_fp, cmap="gray")
    plt.axis("off")
    plt.title(f"{scanner} Aggregated Fingerprint")
    plt.savefig(os.path.join(output_folder, f"{scanner}_aggregated.png"), bbox_inches="tight")
    plt.close()

# Step 5: Save clean dataset CSV
dataset = []
for scanner, agg_fp in aggregated_results.items():
    dataset.append({
        "Scanner": scanner,
        "Fingerprint_File": os.path.join(output_folder, f"{scanner}_aggregated.npy"),
        "Image_File": os.path.join(output_folder, f"{scanner}_aggregated.png")
    })

df = pd.DataFrame(dataset)
csv_path = os.path.join(output_folder, "Aggregated_Scanner_Fingerprints.csv")
df.to_csv(csv_path, index=False)

print(f"‚úÖ Preprocessing complete! Aggregated fingerprints saved in {output_folder}")
print(f"‚úÖ Clean dataset CSV: {csv_path}")

import os

# Base dataset directory
base_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"

# Old ‚Üí New folder name mapping
rename_map = {
    "Objective1_PRNU_Output": "PRNU_Output",
    "Objective2_PRNU_Preprocessed": "PRNU_Preprocessed"
}

# Perform renaming
for old_name, new_name in rename_map.items():
    old_path = os.path.join(base_dir, old_name)
    new_path = os.path.join(base_dir, new_name)

    if os.path.exists(old_path):
        os.rename(old_path, new_path)
        print(f"‚úÖ Renamed: {old_name} ‚Üí {new_name}")
    else:
        print(f"‚ö†Ô∏è Folder not found: {old_name}")

"""# **View Preprocessed file**"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Path to preprocessed dataset
preprocessed_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed"

# --- 1. View CSV summary ---
csv_path = os.path.join(preprocessed_dir, "Aggregated_Scanner_Fingerprints.csv")
if os.path.exists(csv_path):
    df = pd.read_csv(csv_path)
    print("‚úÖ Aggregated Scanner Fingerprints CSV:")
    display(df.head())
else:
    print("‚ö†Ô∏è CSV not found at:", csv_path)

# --- 2. View aggregated PRNU .npy files ---
print("\n‚úÖ Available aggregated .npy files:")
npy_files = [f for f in os.listdir(preprocessed_dir) if f.endswith(".npy")]
print(npy_files)

for f in npy_files:
    scanner_name = f.replace("_aggregated.npy", "")
    npy_path = os.path.join(preprocessed_dir, f)

    # Load fingerprint
    fingerprint = np.load(npy_path)

    # Plot
    plt.figure(figsize=(4, 4))
    plt.imshow(fingerprint, cmap="gray")
    plt.title(f"Aggregated Fingerprint - {scanner_name}")
    plt.axis("off")
    plt.show()

# --- 3. View pre-saved .png visualizations ---
print("\n‚úÖ Available .png preview images:")
png_files = [f for f in os.listdir(preprocessed_dir) if f.endswith(".png")]
print(png_files)

for f in png_files:
    img_path = os.path.join(preprocessed_dir, f)
    img = plt.imread(img_path)

    plt.figure(figsize=(4, 4))
    plt.imshow(img, cmap="gray")
    plt.title(f.replace("_aggregated.png", ""))
    plt.axis("off")
    plt.show()

"""# **Compare Raw PRNU and Preprocessed Image**"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from skimage.metrics import structural_similarity as ssim

# Paths
raw_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Output"
pre_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed"

def compare_all_scanner_images(scanner_name, max_show=5):
    """
    Compare ALL raw PRNU files vs preprocessed PRNU for a given scanner.
    Shows difference heatmap + similarity metrics (SSIM, Correlation, MSE).
    """
    # Find all raw files for the scanner
    raw_files = [f for f in os.listdir(raw_dir) if f.startswith(scanner_name) and f.endswith(".npy")]
    if not raw_files:
        print(f"‚ö†Ô∏è No raw files found for {scanner_name} in {raw_dir}")
        return

    # Preprocessed aggregated file
    pre_file = f"{scanner_name}_aggregated.npy"
    pre_path = os.path.join(pre_dir, pre_file)
    if not os.path.exists(pre_path):
        print(f"‚ö†Ô∏è No preprocessed file found for {scanner_name} in {pre_dir}")
        return

    pre_fp = np.load(pre_path)

    # Loop over each raw file
    for i, raw_file in enumerate(raw_files[:max_show]):  # limit to `max_show` files
        raw_path = os.path.join(raw_dir, raw_file)
        raw_fp = np.load(raw_path)

        # ‚úÖ Resize preprocessed fingerprint to raw size
        pre_resized = cv2.resize(pre_fp, (raw_fp.shape[1], raw_fp.shape[0]))

        # Difference map (normalized for visualization)
        diff = raw_fp - pre_resized
        diff_norm = (diff - diff.min()) / (diff.max() - diff.min() + 1e-8)

        # --- üî¢ Similarity Metrics ---
        raw_norm = (raw_fp - raw_fp.min()) / (raw_fp.max() - raw_fp.min() + 1e-8)
        pre_norm = (pre_resized - pre_resized.min()) / (pre_resized.max() - pre_resized.min() + 1e-8)

        # SSIM
        ssim_score = ssim(raw_norm, pre_norm, data_range=1.0)

        # Correlation
        corr = np.corrcoef(raw_norm.flatten(), pre_norm.flatten())[0, 1]

        # MSE
        mse = np.mean((raw_norm - pre_norm) ** 2)

        # --- Plot ---
        plt.figure(figsize=(15, 5))
        plt.suptitle(f"{scanner_name} - {raw_file}\nSSIM={ssim_score:.3f}, Corr={corr:.3f}, MSE={mse:.5f}", fontsize=12)

        # Raw PRNU
        plt.subplot(1, 3, 1)
        plt.imshow(raw_fp, cmap="gray")
        plt.title("Raw PRNU")
        plt.axis("off")

        # Preprocessed PRNU (resized)
        plt.subplot(1, 3, 2)
        plt.imshow(pre_resized, cmap="gray")
        plt.title("Preprocessed (Resized)")
        plt.axis("off")

        # Difference Heatmap
        plt.subplot(1, 3, 3)
        plt.imshow(diff_norm, cmap="hot")
        plt.title("Difference Heatmap\n(Raw - Preprocessed)")
        plt.axis("off")

        plt.show()

# üî• Example usage:
compare_all_scanner_images("Canon120", max_show=3)
compare_all_scanner_images("EpsonV39", max_show=3)

# Correct paths based on your dataset
BASE_DIR   = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
PRNU_PRE   = os.path.join(BASE_DIR, "PRNU_Preprocessed")
DOCS_DIR   = os.path.join(BASE_DIR, "Original_documents")

# Load fingerprints
scanner_fps = {}
for file in os.listdir(PRNU_PRE):
    if file.endswith(".npy"):
        scanner_name = file.split("_")[0]   # e.g., "Canon120_aggregated.npy" ‚Üí "Canon120"
        scanner_fps[scanner_name] = np.load(os.path.join(PRNU_PRE, file))

"""# **Extract Noise Residual**"""

import os
import cv2
import numpy as np

# =========================
# PATH SETUP
# =========================
BASE_DIR   = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
PRNU_PRE   = os.path.join(BASE_DIR, "PRNU_Preprocessed")  # preprocessed fingerprints
SOURCE_FOLDERS = ["Official", "Wikipedia"]                 # source folders with scanner subfolders

# =========================
# Step 1: Load Preprocessed Fingerprints
# =========================
scanner_fps = {}
for file in os.listdir(PRNU_PRE):
    if file.endswith(".npy"):
        scanner_name = file.split("_")[0]   # "Canon120_aggregated.npy" ‚Üí "Canon120"
        scanner_fps[scanner_name] = np.load(os.path.join(PRNU_PRE, file))

print(f"‚úÖ Loaded PRNU fingerprints for scanners: {list(scanner_fps.keys())}")

# =========================
# Step 2: Extract Noise Residual
# =========================
def extract_noise_residual(image_path):
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return None
    img = cv2.resize(img, (256, 256))
    denoised = cv2.GaussianBlur(img, (3, 3), 0)
    residual = img.astype(np.float32) - denoised.astype(np.float32)
    residual /= (np.std(residual) + 1e-8)
    return residual

# =========================
# Step 3: Match Document to Scanner
# =========================
def match_scanner(image_path, scanner_fps):
    residual = extract_noise_residual(image_path)
    if residual is None:
        return None, None

    best_match = None
    best_score = -1
    for scanner, fp in scanner_fps.items():
        # Resize fingerprint if needed
        if fp.shape != residual.shape:
            fp_resized = cv2.resize(fp, (residual.shape[1], residual.shape[0]))
        else:
            fp_resized = fp
        score = np.corrcoef(residual.flatten(), fp_resized.flatten())[0,1]
        if score > best_score:
            best_score = score
            best_match = scanner
    return best_match, best_score

# =========================
# Step 4: Loop Through All Scanner Documents
# =========================
for src_folder in SOURCE_FOLDERS:
    src_path = os.path.join(BASE_DIR, src_folder)

    for scanner_name in os.listdir(src_path):
        scanner_folder = os.path.join(src_path, scanner_name)
        if not os.path.isdir(scanner_folder):
            continue

        print(f"\nüìÇ Processing documents for scanner: {scanner_name} ({src_folder})")
        for file in os.listdir(scanner_folder):
            if file.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', '.tif')):
                doc_path = os.path.join(scanner_folder, file)
                predicted_scanner, score = match_scanner(doc_path, scanner_fps)
                if predicted_scanner:
                    print(f"{file} ‚Üí Predicted: {predicted_scanner}, Correlation: {score:.4f}")
                else:
                    print(f"{file} ‚Üí Could not process document.")

import numpy as np
import pandas as pd
import os

pre_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed"
output_csv = os.path.join(pre_dir, "Aggregated_Scanner_Fingerprints_Features.csv")

features_list = []
for f in os.listdir(pre_dir):
    if f.endswith(".npy") and "aggregated" in f:
        scanner_name = f.replace("_aggregated.npy","")
        fp = np.load(os.path.join(pre_dir, f))

        # Compute basic PRNU features
        features = {
            "Scanner": scanner_name,
            "Mean": fp.mean(),
            "StdDev": fp.std(),
            "Energy": np.sum(fp**2),
            "Skewness": np.mean((fp - fp.mean())**3) / (fp.std()**3 + 1e-8),
            "Kurtosis": np.mean((fp - fp.mean())**4) / (fp.std()**4 + 1e-8),
            "Entropy": -np.sum(fp*np.log2(np.abs(fp)+1e-12))
        }
        features_list.append(features)

df_features = pd.DataFrame(features_list)
df_features.to_csv(output_csv, index=False)
print("‚úÖ Saved numeric feature CSV:", output_csv)

"""# **EDA On PRNU Preprocessed**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ===============================
# Load preprocessed features CSV
# ===============================
base_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed"
features_csv = os.path.join(base_dir, "Aggregated_Scanner_Fingerprints_Features.csv")

df = pd.read_csv(features_csv)
print("‚úÖ Loaded features CSV:")
display(df)

# ===============================
# 1. Basic Statistics
# ===============================
print("\nüìä Summary Statistics:")
display(df.describe())

# ===============================
# 2. Distribution of each numeric feature
# ===============================
numeric_cols = ["Mean", "StdDev", "Energy", "Skewness", "Kurtosis", "Entropy"]

plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[col], kde=True, bins=15, color='skyblue')
    plt.title(f"Distribution of {col}")
plt.tight_layout()
plt.show()

# ===============================
# 3. Boxplots per feature (scanner comparison)
# ===============================
plt.figure(figsize=(18, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(2, 3, i)
    sns.boxplot(x="Scanner", y=col, data=df, palette="Set3")
    plt.xticks(rotation=45)
    plt.title(f"{col} per Scanner")
plt.tight_layout()
plt.show()

# ===============================
# 4. Pairplot to see relationships
# ===============================
sns.pairplot(df, hue="Scanner", vars=numeric_cols, height=2.5, palette="tab10")
plt.suptitle("Pairplot of PRNU Features by Scanner", y=1.02)
plt.show()

# ===============================
# 5. Correlation heatmap
# ===============================
plt.figure(figsize=(8, 6))
corr_matrix = df[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap of PRNU Features")
plt.show()

# ===============================
# Optional: Scatterplots of interesting feature pairs
# ===============================
plt.figure(figsize=(10, 6))
sns.scatterplot(x="Mean", y="Energy", hue="Scanner", data=df, palette="tab10", s=100)
plt.title("Mean vs Energy per Scanner")
plt.show()

"""# **Mean	Median	StdDev	Skewness**"""

import pandas as pd
import os
from scipy.stats import skew, kurtosis

# ===============================
# Load preprocessed features CSV
# ===============================
base_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed"
features_csv = os.path.join(base_dir, "Aggregated_Scanner_Fingerprints_Features.csv")

df = pd.read_csv(features_csv)
print("‚úÖ Loaded features CSV:")
display(df)

# ===============================
# Compute descriptive statistics per scanner
# ===============================
numeric_cols = ["Mean", "StdDev", "Energy", "Skewness", "Kurtosis", "Entropy"]

# Function to compute additional stats
def compute_extra_stats(group):
    return pd.Series({
        'Mean': group['Mean'].mean(),
        'Median': group['Mean'].median(),
        'Std_Dev': group['Mean'].std(),
        'Min': group['Mean'].min(),
        'Max': group['Mean'].max(),
        'Skewness': skew(group['Mean']),
        'Kurtosis': kurtosis(group['Mean']),
        'Range': group['Mean'].max() - group['Mean'].min()
    })

# Apply to each scanner
stats_df = df.groupby("Scanner").apply(compute_extra_stats).reset_index()

print("\nüìä Scanner-wise Extended Statistics:")
display(stats_df)

# Optional: Save to CSV
stats_csv = os.path.join(base_dir, "Scanner_Features_Extended_Statistics.csv")
stats_df.to_csv(stats_csv, index=False)
print(f"‚úÖ Saved extended statistics CSV: {stats_csv}")

"""# **Total Samples**"""

import os
import numpy as np
import pandas as pd

PRNU_FOLDER = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed/All_Samples"
OUTPUT_CSV = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed/Final_Scanner_Features_AllSamples.csv"

results = []

# Process each .npy PRNU sample
for file in os.listdir(PRNU_FOLDER):
    if file.endswith(".npy"):
        scanner_name = file.split("_fingerprint")[0]
        prnu_path = os.path.join(PRNU_FOLDER, file)
        prnu_data = np.load(prnu_path)

        # Calculate features
        mean_val = np.mean(prnu_data)
        median_val = np.median(prnu_data)
        std_val = np.std(prnu_data)
        energy_val = np.sum(prnu_data**2)
        skew_val = np.mean((prnu_data - mean_val)**3) / (std_val**3 + 1e-8)
        kurt_val = np.mean((prnu_data - mean_val)**4) / (std_val**4 + 1e-8)
        entropy_val = -np.sum(prnu_data * np.log2(np.abs(prnu_data)+1e-8))
        range_val = np.max(prnu_data) - np.min(prnu_data)

        results.append({
            "Scanner": scanner_name,
            "Mean": mean_val,
            "Median": median_val,
            "StdDev": std_val,
            "Energy": energy_val,
            "Skewness": skew_val,
            "Kurtosis": kurt_val,
            "Entropy": entropy_val,
            "Range": range_val
        })

# Save all features to CSV
df = pd.DataFrame(results)
df.to_csv(OUTPUT_CSV, index=False)
print(f"‚úÖ Features for ALL scanner samples saved to: {OUTPUT_CSV}")
print(f"Total samples processed: {len(df)}")

import os

# Root folder
ROOT_DIR = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"

# Folders to count files from
FOLDERS = ["PRNU_Fingerprints", "Official", "Wikipedia"]

# Image file extensions to consider
IMAGE_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp', '.npy')

grand_total = 0
samples_per_folder = {}

for folder in FOLDERS:
    folder_path = os.path.join(ROOT_DIR, folder)

    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è Folder not found: {folder_path}")
        continue

    total_files = 0
    # Loop through all subfolders and nested subfolders
    for dirpath, dirnames, filenames in os.walk(folder_path):
        total_files += len([f for f in filenames if f.lower().endswith(IMAGE_EXTENSIONS)])

    samples_per_folder[folder] = total_files
    grand_total += total_files
    print(f"Folder: {folder} -> Total samples: {total_files}")

print(f"\n‚úÖ Grand total of all samples across all folders: {grand_total}")

# Install dependencies for PDF reading
!pip install pdf2image
!apt-get install poppler-utils -y

import os
from skimage import io
from pdf2image import convert_from_path

# -------------------------------
# Paths
# -------------------------------
ROOT_DIR = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
FOLDERS = ["Official", "Wikipedia"]  # Only use these for training

# -------------------------------
# Supported formats
# -------------------------------
IMAGE_EXTS = ['tif', 'tiff', 'jpg', 'jpeg', 'png']
PDF_EXTS = ['pdf']

# -------------------------------
# Count samples
# -------------------------------
total_samples = 0
samples_per_folder = {}

for folder in FOLDERS:
    folder_path = os.path.join(ROOT_DIR, folder)
    folder_count = 0

    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è Folder not found: {folder_path}")
        continue

    for dirpath, _, filenames in os.walk(folder_path):
        for f in filenames:
            ext = f.lower().split('.')[-1]
            if ext in IMAGE_EXTS + PDF_EXTS:
                folder_count += 1

    samples_per_folder[folder] = folder_count
    total_samples += folder_count

# -------------------------------
# Display results
# -------------------------------
for folder, count in samples_per_folder.items():
    print(f"Folder: {folder} -> Total samples: {count}")

print(f"\n‚úÖ Grand total of all samples across all folders: {total_samples}")

"""# **Feature Extraction from wikipedia and Official samples**"""

import os
import numpy as np
import pandas as pd
from skimage import io
from pdf2image import convert_from_path
from scipy.stats import skew, kurtosis, entropy
from tqdm import tqdm

# -------------------------------
# Paths
# -------------------------------
ROOT_DIR = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
FOLDERS = ["Official", "Wikipedia"]
OUTPUT_CSV = os.path.join(ROOT_DIR, "All_Samples_PRNU_Features_Advanced.csv")

# -------------------------------
# Supported formats
# -------------------------------
IMAGE_EXTS = ['tif', 'tiff', 'jpg', 'jpeg', 'png']
PDF_EXTS = ['pdf']

# -------------------------------
# Feature extraction function
# -------------------------------
def extract_prnu_features(image_path):
    """
    Compute advanced PRNU statistical features:
    Mean, Median, StdDev, Energy, Range, Skewness, Kurtosis, Entropy
    """
    ext = image_path.lower().split('.')[-1]

    # Load PDF as image if needed
    if ext in PDF_EXTS:
        pages = convert_from_path(image_path, dpi=200)
        img = np.array(pages[0].convert('L'), dtype=np.float32)  # first page grayscale
    else:
        img = io.imread(image_path).astype(np.float32)
        # Convert RGB to grayscale
        if img.ndim == 3:
            img = np.mean(img, axis=2)

    flat = img.flatten()
    mean_val = np.mean(flat)
    median_val = np.median(flat)
    std_val = np.std(flat)
    energy_val = np.sum(flat**2)
    range_val = np.max(flat) - np.min(flat)
    skew_val = skew(flat)
    kurt_val = kurtosis(flat)
    # Normalize histogram for entropy calculation
    hist, _ = np.histogram(flat, bins=256, range=(flat.min(), flat.max()), density=True)
    hist += 1e-12  # avoid log(0)
    entropy_val = -np.sum(hist * np.log2(hist))

    return [mean_val, median_val, std_val, energy_val, range_val, skew_val, kurt_val, entropy_val]

# -------------------------------
# Loop through folders and extract features
# -------------------------------
all_features = []
all_labels = []

for folder in FOLDERS:
    folder_path = os.path.join(ROOT_DIR, folder)

    if not os.path.exists(folder_path):
        print(f"‚ö†Ô∏è Folder not found: {folder_path}")
        continue

    # Walk through subfolders
    for dirpath, _, filenames in os.walk(folder_path):
        for f in tqdm(filenames, desc=f"Processing {folder}", unit="file"):
            ext = f.lower().split('.')[-1]
            if ext in IMAGE_EXTS + PDF_EXTS:
                file_path = os.path.join(dirpath, f)
                try:
                    features = extract_prnu_features(file_path)
                    all_features.append(features)
                    # Subfolder name as scanner label
                    label = os.path.basename(dirpath)
                    all_labels.append(label)
                except Exception as e:
                    print(f"‚ùå Error processing {file_path}: {e}")

# -------------------------------
# Save to CSV
# -------------------------------
columns = ['Mean', 'Median', 'StdDev', 'Energy', 'Range', 'Skewness', 'Kurtosis', 'Entropy']
df = pd.DataFrame(all_features, columns=columns)
df['Scanner'] = all_labels
df.to_csv(OUTPUT_CSV, index=False)

print(f"‚úÖ Features for all samples saved to: {OUTPUT_CSV}")
print(f"Total samples processed: {len(df)}")

import torch

if torch.cuda.is_available():
    print("‚úÖ GPU is available:", torch.cuda.get_device_name(0))
else:
    print("‚ö†Ô∏è GPU not available")

"""# **Total samples in CSV**"""

import pandas as pd
import os

# Path to the features CSV
ROOT_DIR = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
FEATURES_CSV = os.path.join(ROOT_DIR, "All_Samples_PRNU_Features_Advanced.csv")

# Check if CSV exists
if not os.path.exists(FEATURES_CSV):
    print(f"‚ö†Ô∏è CSV file not found: {FEATURES_CSV}")
else:
    # Load CSV
    df = pd.read_csv(FEATURES_CSV)
    print(f"‚úÖ CSV loaded: {FEATURES_CSV}")

    # Total samples and columns
    total_samples = len(df)
    total_features = df.shape[1] - 1  # subtract 'Scanner' column

    print(f"Total samples (images) in CSV: {total_samples}")
    print(f"Number of features per sample: {total_features}")

    # Optional: show sample labels count
    print("\nSamples per scanner:")
    print(df['Scanner'].value_counts())

"""# **Model Building**

# **i) SVM Model**
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# -------------------------------
# 1. Load CSV
# -------------------------------
CSV_PATH = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/All_Samples_PRNU_Features_Advanced.csv"
df = pd.read_csv(CSV_PATH)

print(f"‚úÖ CSV loaded: {CSV_PATH}")
print(f"Total samples: {len(df)}")
print(f"Features per sample: {df.shape[1]-1}")  # minus label column

# -------------------------------
# 2. Prepare data
# -------------------------------
X = df.drop('Scanner', axis=1).values
y = df['Scanner'].values

# Encode scanner labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------
# 3. Train/Test split
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# -------------------------------
# 4. Train SVM
# -------------------------------
svm_model = SVC(kernel='linear', C=1.0, probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# -------------------------------
# 5. Evaluate
# -------------------------------
y_pred = svm_model.predict(X_test)
target_names = [str(c) for c in le.classes_]

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=target_names))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

accuracy = np.mean(y_pred == y_test)
print(f"‚úÖ Accuracy: {accuracy*100:.2f}%")

# -------------------------------
# 6. Save model, scaler, encoder
# -------------------------------
MODEL_PATH = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed/SVM_Scanner_Model.joblib"
SCALER_PATH = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed/Scaler.joblib"
LE_PATH = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Preprocessed/LabelEncoder.joblib"

joblib.dump(svm_model, MODEL_PATH)
joblib.dump(scaler, SCALER_PATH)
joblib.dump(le, LE_PATH)
print("‚úÖ Model, scaler, and label encoder saved!")

"""**Model's Accuracy**"""

from sklearn.metrics import accuracy_score

# -------------------------------
# Case 1: Accuracy on a test set
# -------------------------------
# y_test = true labels for test data
# y_pred = predictions from your model

acc_test = accuracy_score(y_test, y_pred)
print(f"‚úÖ Test set Accuracy: {acc_test*100:.2f}%")

# -------------------------------
# Case 2: Accuracy on entire dataset
# -------------------------------
# X_scaled = features (scaled) for all samples
# y_encoded = true labels (encoded) for all samples
# svm_model = your trained SVM model

y_pred_all = svm_model.predict(X_scaled)
acc_all = accuracy_score(y_encoded, y_pred_all)
print(f"‚úÖ Overall Accuracy on all samples: {acc_all*100:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

# y_test: true labels for test set
# y_pred: predicted labels for test set
# le.classes_: label encoder classes

# Convert numeric labels to strings for display
target_names = [str(c) for c in le.classes_]

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Convert counts to percentages
cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

# Plot heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_percent, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=target_names, yticklabels=target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix (%) - Test Set")
plt.show()

"""# **ii) XGBoost Model**"""

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

# -------------------------------
# Paths
# -------------------------------
ROOT_DIR = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
CSV_PATH = os.path.join(ROOT_DIR, "All_Samples_PRNU_Features_Advanced.csv")
MODEL_PATH = os.path.join(ROOT_DIR, "PRNU_Preprocessed/XGB_Scanner_Model.joblib")
SCALER_PATH = os.path.join(ROOT_DIR, "PRNU_Preprocessed/Scaler_XGB.joblib")
LE_PATH = os.path.join(ROOT_DIR, "PRNU_Preprocessed/LabelEncoder_XGB.joblib")

# -------------------------------
# Load CSV
# -------------------------------
df = pd.read_csv(CSV_PATH)
print(f"‚úÖ CSV loaded: {CSV_PATH}")
print(f"Total samples: {len(df)}, Features per sample: {df.shape[1]-1}")

# Features and labels
X = df.drop(columns=['Scanner']).values
y = df['Scanner'].values

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------
# Train/Test split
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# -------------------------------
# Train XGBoost model
# -------------------------------
xgb_model = XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    objective='multi:softmax',
    num_class=len(le.classes_),
    eval_metric='mlogloss',
    use_label_encoder=False,
    n_jobs=-1,
    random_state=42
)

xgb_model.fit(X_train, y_train)

# -------------------------------
# Evaluate
# -------------------------------
y_pred = xgb_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"‚úÖ Test Accuracy: {acc*100:.2f}%")
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# -------------------------------
# Save Model, Scaler, LabelEncoder
# -------------------------------
joblib.dump(xgb_model, MODEL_PATH)
joblib.dump(scaler, SCALER_PATH)
joblib.dump(le, LE_PATH)
print(f"‚úÖ XGBoost model, scaler, and label encoder saved!")

"""**Model's Accuracy**"""

from sklearn.metrics import accuracy_score

# -------------------------------
# Case 1: Accuracy on a test set
# -------------------------------
# y_test = true labels for test data
# y_pred_test = predictions from your XGBoost model

y_pred_test = xgb_model.predict(X_test)
acc_test = accuracy_score(y_test, y_pred_test)
print(f"‚úÖ Test set Accuracy: {acc_test*100:.2f}%")

# -------------------------------
# Case 2: Accuracy on entire dataset
# -------------------------------
# X_scaled = features (scaled) for all samples
# y_encoded = true labels (encoded) for all samples
# xgb_model = your trained XGBoost model

y_pred_all = xgb_model.predict(X_scaled)
acc_all = accuracy_score(y_encoded, y_pred_all)
print(f"‚úÖ Overall Accuracy on all samples: {acc_all*100:.2f}%")

"""# **iii) Random Forest Model**"""

import os
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# -------------------------------
# Paths
# -------------------------------
ROOT_DIR = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
CSV_PATH = os.path.join(ROOT_DIR, "All_Samples_PRNU_Features_Advanced.csv")
MODEL_PATH = os.path.join(ROOT_DIR, "PRNU_Preprocessed/RF_Scanner_Model.joblib")
SCALER_PATH = os.path.join(ROOT_DIR, "PRNU_Preprocessed/Scaler_RF.joblib")
LE_PATH = os.path.join(ROOT_DIR, "PRNU_Preprocessed/LabelEncoder_RF.joblib")

# -------------------------------
# Load CSV
# -------------------------------
df = pd.read_csv(CSV_PATH)
print(f"‚úÖ CSV loaded: {CSV_PATH}")
print(f"Total samples: {len(df)}, Features per sample: {df.shape[1]-1}")

# Features and labels
X = df.drop(columns=['Scanner']).values
y = df['Scanner'].values

# Encode labels
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# -------------------------------
# Train/Test Split
# -------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# -------------------------------
# Train Random Forest model
# -------------------------------
rf_model = RandomForestClassifier(
    n_estimators=200,       # number of trees
    max_depth=None,         # expand until all leaves are pure
    random_state=42,
    n_jobs=-1               # use all cores for faster training
)
rf_model.fit(X_train, y_train)

# -------------------------------
# Evaluate
# -------------------------------
y_pred = rf_model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print(f"‚úÖ Random Forest Accuracy: {acc*100:.2f}%\n")

print("Classification Report:")
print(classification_report(y_test, y_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# -------------------------------
# Save model, scaler, and label encoder
# -------------------------------
joblib.dump(rf_model, MODEL_PATH)
joblib.dump(scaler, SCALER_PATH)
joblib.dump(le, LE_PATH)
print(f"‚úÖ Random Forest model, scaler, and label encoder saved!")

# ===========================================
# Step 1: Mount Google Drive
# ===========================================
from google.colab import drive
drive.mount('/content/drive')

# ===========================================
# Step 2: Import libraries
# ===========================================
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization

# ===========================================
# Step 3: Load CSV
# ===========================================
csv_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/All_Samples_PRNU_Features_Advanced.csv"
data = pd.read_csv(csv_path)

print(f"‚úÖ CSV loaded: {csv_path}")
print(f"Total samples: {len(data)}, Features per sample: {data.shape[1]-1}")
print("\nSamples per scanner:")
print(data['Scanner'].value_counts())

# ===========================================
# Step 4: Prepare features (X) and labels (y)
# ===========================================
X = data.drop(columns=['Scanner']).values
y = data['Scanner'].values

# Encode labels (150, 300 ‚Üí 0,1)
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# ===========================================
# Step 5: Build CNN-like fully connected network
# ===========================================
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    BatchNormalization(),
    Dropout(0.3),

    Dense(64, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),

    Dense(32, activation='relu'),
    Dense(len(np.unique(y_encoded)), activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# ===========================================
# Step 6: Train model
# ===========================================
history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=30,
    batch_size=32,
    verbose=1
)

# ===========================================
# Step 7: Evaluate on test set
# ===========================================
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\n‚úÖ Test Accuracy: {test_acc*100:.2f}%")

# ===========================================
# Step 8: Save model
# ===========================================
model.save("/content/drive/MyDrive/AI_Trace_Finder_Datasets/Scanner_CNN_Model.h5")
print("‚úÖ Model saved to Google Drive!")

# ===========================================
# Step 1: Mount Google Drive
# ===========================================
from google.colab import drive
drive.mount('/content/drive')

# ===========================================
# Step 2: Import libraries
# ===========================================
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
import os

# ===========================================
# Step 3: Paths to dataset folders
# ===========================================
dataset_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets"
official_path = os.path.join(dataset_dir, "Official")
wiki_path = os.path.join(dataset_dir, "Wikipedia")

print("üìÇ Dataset folders:")
print("Official:", official_path)
print("Wikipedia:", wiki_path)

# ===========================================
# Step 4: Image Data Generator (Train/Test Split)
# ===========================================
img_size = (128, 128)   # resize all images
batch_size = 32

datagen = ImageDataGenerator(
    rescale=1./255,          # normalize pixels
    validation_split=0.2     # 80/20 train-test split
)

train_gen = datagen.flow_from_directory(
    dataset_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

val_gen = datagen.flow_from_directory(
    dataset_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

print("‚úÖ Classes:", train_gen.class_indices)

# ===========================================
# Step 5: Build CNN Model
# ===========================================
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)),
    MaxPooling2D((2,2)),
    BatchNormalization(),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    BatchNormalization(),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    BatchNormalization(),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_gen.num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# ===========================================
# Step 6: Train Model
# ===========================================
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=20,
    verbose=1
)

# ===========================================
# Step 7: Evaluate Model
# ===========================================
loss, acc = model.evaluate(val_gen, verbose=0)
print(f"\n‚úÖ Validation Accuracy: {acc*100:.2f}%")

# ===========================================
# Step 8: Save Model
# ===========================================
save_path = os.path.join(dataset_dir, "Scanner_Image_CNN.h5")
model.save(save_path)
print(f"‚úÖ Model saved at {save_path}")

"""# **SVM Model using Sample Images**"""

import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Paths
official_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Official"
wiki_dir = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Wikipedia"

# Parameters
img_size = (128, 128)  # Resize for consistency
data = []
labels = []

# Function to load images from nested folders
def load_images_from_dir(base_dir, label_prefix):
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.lower().endswith((".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff")):
                img_path = os.path.join(root, file)
                try:
                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                    img = cv2.resize(img, img_size)
                    data.append(img.flatten())  # Flatten for SVM
                    # Label = folder name (scanner model)
                    labels.append(os.path.basename(root))
                except Exception as e:
                    print(f"‚ö†Ô∏è Skipping {img_path}: {e}")

# Load datasets
print("üìÇ Loading Official dataset...")
load_images_from_dir(official_dir, "official")

print("üìÇ Loading Wikipedia dataset...")
load_images_from_dir(wiki_dir, "wiki")

# Convert to numpy
X = np.array(data)
y = np.array(labels)

print(f"‚úÖ Loaded {len(X)} images. Feature shape: {X.shape}")

# Step 4: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Step 5: Train SVM
print("üöÄ Training SVM model...")
svm_model = SVC(kernel="linear", probability=True)
svm_model.fit(X_train, y_train)

# Step 6: Evaluate
y_pred = svm_model.predict(X_test)
print("\nüéØ Accuracy:", accuracy_score(y_test, y_pred))
print("\nüìä Classification Report:\n", classification_report(y_test, y_pred))

import os
import cv2
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# ===============================
# Step 1: Load Images
# ===============================
def load_images_from_folder(folder, label):
    images, labels = [], []
    for fname in os.listdir(folder):
        fpath = os.path.join(folder, fname)
        try:
            img = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                img = cv2.resize(img, (128, 128))  # resize for uniformity
                images.append(img)
                labels.append(label)
        except Exception as e:
            print(f"‚ö†Ô∏è Skipped {fname}: {e}")
    return images, labels

official_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Official"
wiki_path = "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Wikipedia"

print("üìÇ Loading Official dataset...")
X_official, y_official = load_images_from_folder(official_path, label=0)

print("üìÇ Loading Wikipedia dataset...")
X_wiki, y_wiki = load_images_from_folder(wiki_path, label=1)

X = np.array(X_official + X_wiki)
y = np.array(y_official + y_wiki)

print(f"‚úÖ Loaded {len(X)} images. Shape: {X.shape}")

# ===============================
# Step 2: Extract PRNU-like noise residuals
# ===============================
def extract_prnu(img):
    # Apply Gaussian blur (suppress content)
    denoised = cv2.GaussianBlur(img, (3, 3), 0)
    # Residual (image - denoised) ~ PRNU
    noise = img - denoised
    return noise.flatten()

print("üîç Extracting PRNU-like features...")
features = np.array([extract_prnu(img) for img in X])
print(f"‚úÖ Feature matrix shape: {features.shape}")

# ===============================
# Step 3: Dimensionality Reduction
# ===============================
print("‚ö° Applying PCA to reduce noise dimensions...")
pca = PCA(n_components=300)  # Keep 300 principal components
features_pca = pca.fit_transform(features)
print(f"‚úÖ Reduced feature shape: {features_pca.shape}")

# ===============================
# Step 4: Train/Test Split
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    features_pca, y, test_size=0.2, random_state=42, stratify=y
)

# ===============================
# Step 5: Train SVM
# ===============================
print("üöÄ Training SVM model...")
svm = SVC(kernel="linear", C=1.0)
svm.fit(X_train, y_train)

# ===============================
# Step 6: Evaluate
# ===============================
y_pred = svm.predict(X_test)

print("\nüéØ Accuracy:", accuracy_score(y_test, y_pred))
print("\nüìä Classification Report:\n", classification_report(y_test, y_pred))

# ===============================
# Step 7: Save Features (optional)
# ===============================
features_df = pd.DataFrame(features_pca)
features_df["label"] = y
features_df.to_csv("/content/drive/MyDrive/AI_Trace_Finder_Datasets/PRNU_Features.csv", index=False)
print("üíæ Features saved to PRNU_Features.csv")

import os, cv2
import numpy as np
from sklearn.model_selection import train_test_split

def load_images_from_folder(folder, label):
    images, labels = [], []
    valid_ext = (".jpg", ".jpeg", ".png", ".tif", ".bmp")

    for root, _, files in os.walk(folder):  # recursive
        for file in files:
            if file.lower().endswith(valid_ext):
                path = os.path.join(root, file)
                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
                if img is not None:
                    img = cv2.resize(img, (128, 128))  # resize
                    images.append(img.flatten())       # flatten to 1D
                    labels.append(label)
    return images, labels

print("üìÇ Loading Official dataset...")
X_official, y_official = load_images_from_folder(
    "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Official", label=150
)
print(f"‚úÖ Loaded {len(X_official)} Official images")

print("üìÇ Loading Wikipedia dataset...")
X_wiki, y_wiki = load_images_from_folder(
    "/content/drive/MyDrive/AI_Trace_Finder_Datasets/Wikipedia", label=300
)
print(f"‚úÖ Loaded {len(X_wiki)} Wikipedia images")

# Merge
X = np.array(X_official + X_wiki)
y = np.array(y_official + y_wiki)

print(f"üéØ Final dataset size: {X.shape}, Labels: {len(y)}")

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)